@article{Oniga2015,
	abstract = {This paper introduces a computationally efficient, approach for obstacle detection in driving assistance applications, based on stereovision. The proposed approach involves three different steps aiming an increased quality of the results. The first step relies on the basics: obstacles correspond to peak regions in the u-disparity map. By applying the model of the stereo system, the peaks are detected with an adaptive threshold. The adaptive thresholding will calculate the accumulation of points required for an obstacle based on its distance (disparity) and will be related to the characteristics of the stereovision system. The second and third steps consist of refining the result of the previous, vertically respectively horizontally. This is necessary in order to fill out unmarked pixel regions which are classified as belonging to obstacles. The second step iterates vertically and propagates the obstacle label to neighbor pixels. The third step improves obstacle regions horizontally, with points that do not belong to the road. The solution is fast and reliable, on various scenarios, as every step is an improvement of the standard U-disparity approach for obstacle detection. {\textcopyright} 2015 IEEE.},
	author = {Oniga, Florin and Sarkozi, Ervin and Nedevschi, Sergiu},
	doi = {10.1109/ICCP.2015.7312630},
	file = {:home/nisaak/Downloads/07312630.pdf:pdf},
	isbn = {9781467382007},
	journal = {Proceedings - 2015 IEEE 11th International Conference on Intelligent Computer Communication and Processing, ICCP 2015},
	keywords = {driving assistance,obstacle detection,stereovision,u-disparity},
	mendeley-groups = {projects/proj-sp-isaak},
	pages = {203--207},
	publisher = {IEEE},
	title = {{Fast obstacle detection using U-disparity maps with stereo vision}},
	year = {2015}
}

@INPROCEEDINGS{V-disparity_nonflat,
	author={R. {Labayrade} and D. {Aubert} and J. -. {Tarel}},
	booktitle={Intelligent Vehicle Symposium, 2002. IEEE},
	title={Real time obstacle detection in stereovision on non flat road geometry through "v-disparity" representation},
	year={2002},
	volume={2},
	number={},
	pages={646-651 vol.2},
	keywords={automotive electronics;computer vision;object detection;real-time systems;stereo image processing;image representation;microcomputer applications;real-time obstacle detection;stereovision;nonflat road geometry;v-disparity representation;fast robust method;uphill gradients;downhill gradients;dynamic vehicle pitching;v-disparity image;semi-global matching;robust obstacle detection;partial occlusion;coherent structures;road edges;lane-markings;stereo image pair;object extraction;tyre-road contact points;Geometry;Robustness;Road vehicles;Image edge detection;Vehicle detection;Vehicle dynamics;Suspensions;Parameter estimation;Layout;Data mining},
	doi={10.1109/IVS.2002.1188024},
	ISSN={},
	month={June},
}

@article{Hu2005,
	abstract = {Reliable understanding of the 3D driving environment is vital for obstacle detection and adaptive cruise control (ACC) applications. Laser or millimeter wave radars have shown good performance in measuring relative speed and distance in a highway driving environment. However the accuracy of these systems decreases in an urban traffic environment as more confusion occurs due to factors such as parked vehicles, guardrails, poles and motorcycles. A stereovision based sensing system provides an effective supplement to radar-based road scene analysis with its much wider field of view and more accurate lateral information. This paper presents an efficient solution using a stereovision based road scene analysis algorithm, which employs the "U-V-disparity" concept. This concept is used to classify a 3D road scene into relative surface planes and characterize the features of road pavement surfaces, roadside structures and obstacles. Real-time implementation of the disparity map calculation and the "U-V-disparity" classification is also presented.},
	author = {Hu, Zhencheng and Uchimura, Keiichi},
	doi = {10.1109/IVS.2005.1505076},
	file = {:home/nisaak/Downloads/01505076.pdf:pdf},
	isbn = {0780389611},
	journal = {IEEE Intelligent Vehicles Symposium, Proceedings},
	keywords = {ACC,Scene analysis,Stereovision,Surface plane extraction,U-V-disparity},
	mendeley-groups = {projects/proj-sp-isaak},
	pages = {48--54},
	title = {{U-V-disparity: An efficient algorithm for stereovision based scene analysis}},
	volume = {2005},
	year = {2005}
}

@article{Kakegawa2018,
	abstract = {In this paper, we propose a road surface segmenta-tion technique which is accurate and suitable for hardware implementation. The road surface is segmented by detecting the boundary between road and obstacle, based on the disparity histogram which we define as VLDH (Vertically Local Disparity Histogram). On each pixel of disparity image, VLDH is computed from the disparities of vertically local neighbourhood pixels. The major advantage is the feasibility of the pipeline processing on image processing hardware for stereo camera. The direct advantage on processing time is confirmed based on implementation into FPGA. Experimental result also shows that the accuracy of the proposed method is better than the conventional method.},
	annote = {promising algorithm for computationally fast implementation. better disparity computation could be implemented to enhance performance. Code only in Matlab and C and not published.
	
	For more on U-V disparity I refer to:
	U-V Disparity Analysis in Urban Environments},
	author = {Kakegawa, Shinji and Matono, Haruki and Kido, Hideaki and Shima, Takeshi},
	doi = {10.1007/s13177-017-0140-8},
	file = {:home/nisaak/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kakegawa et al. - 2018 - Road Surface Segmentation based on Vertically Local Disparity Histogram for Stereo Camera.pdf:pdf},
	issn = {18688659},
	journal = {International Journal of Intelligent Transportation Systems Research},
	keywords = {Disparity histogram,FPGA,Road surface segmentation,Stereo camera,VLDH},
	mendeley-groups = {projects/proj-sp-isaak},
	number = {2},
	pages = {90--97},
	publisher = {International Journal of Intelligent Transportation Systems Research},
	title = {{Road Surface Segmentation based on Vertically Local Disparity Histogram for Stereo Camera}},
	volume = {16},
	year = {2018}
}

@article{Iloie2014,
	abstract = {High accuracy pedestrian detection plays an important role in all intelligent vehicles. This paper describes a system for detecting the obstacles in front of the vehicle and classifying them in pedestrians and non-pedestrians. It acquires the traffic scenes using a low-cost pair of gray intensities stereo cameras. A SORT-SGM stereo-reconstruction technique is used in order to obtain high density and accuracy in stereo-reconstructed points. First, the road plane is computed using the V disparity map and then the obstacles are determined by analyzing the U disparity map. Size related and histogram of oriented gradient based on gray levels features are used for describing each pedestrian hypothesis. A principle component analysis on the features is used for their selection and projection in a relevant space. Different SVM classifiers are trained considering the relevant features on large pedestrian and non-pedestrian image sets. A comparison between them is finally performed for selecting the one that achieves the best classification score.},
	author = {Iloie, Alexandru and Giosan, Ion and Nedevschi, Sergiu},
	doi = {10.1109/ICCP.2014.6936963},
	file = {:home/nisaak/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Iloie, Giosan, Nedevschi - 2014 - UV disparity based obstacle detection and pedestrian classification in urban traffic scenarios.pdf:pdf},
	isbn = {9781479965687},
	journal = {Proceedings - 2014 IEEE 10th International Conference on Intelligent Computer Communication and Processing, ICCP 2014},
	keywords = {UV-disparity,feature extraction,feature selection,obstacle detection,pedestrian classification,road plane detection},
	mendeley-groups = {projects/proj-sp-isaak},
	pages = {119--125},
	title = {{UV disparity based obstacle detection and pedestrian classification in urban traffic scenarios}},
	year = {2014}
}

@article{Ortiz2018,
	author = {Ortiz, Luis E and Cabrera, Elizabeth V and Gonc, Luiz M},
	file = {:home/nisaak/Downloads/elcvia{\_}a2018v17n1p1.pdf:pdf},
	keywords = {depth data,rms error,stereo vision,stereolabs zed},
	mendeley-groups = {projects/proj-sp-isaak},
	number = {1},
	pages = {1--15},
	title = {{Depth Data Error Modeling of the ZED 3D Vision Sensor from Stereolabs}},
	volume = {17},
	year = {2018}
}

@article{Geiger2012,
	abstract = {As a core robotic and vision problem, camera and range sensor calibration have been researched intensely over the last decades. However, robotic research efforts still often get heavily delayed by the requirement of setting up a calibrated system consisting of multiple cameras and range measurement units. With regard to removing this burden, we present a toolbox with web interface for fully automatic camera- to-camera and camera-to-range calibration. Our system is easy to setup and recovers intrinsic and extrinsic camera parameters as well as the transformation between cameras and range sensors within one minute. In contrast to existing calibration approaches, which often require user intervention, the proposed method is robust to varying imaging conditions, fully automatic, and easy to use since a single image and range scan proves sufficient for most calibration scenarios. Experimentally, we demonstrate that the proposed checkerboard corner detector significantly outperforms current state-of-the-art. Furthermore, the proposed camera-to-range registration method is able to discover multiple solutions in the case of ambiguities. Experi- ments using a variety of sensors such as grayscale and color cameras, the Kinect 3D sensor and the Velodyne HDL-64 laser scanner show the robustness of our method in different indoor and outdoor settings and under various lighting conditions. I.},
	author = {Geiger, Andreas and Moosmann, Frank and Car, {\"{O}}mer and Schuster, Bernhard},
	doi = {10.1109/ICRA.2012.6224570},
	file = {:home/nisaak/Downloads/Geiger2012ICRA.pdf:pdf},
	isbn = {9781467314039},
	issn = {10504729},
	journal = {Proceedings - IEEE International Conference on Robotics and Automation},
	mendeley-groups = {projects/proj-sp-isaak},
	pages = {3936--3943},
	title = {{Automatic camera and range sensor calibration using a single shot}},
	year = {2012}
}

@article{Geiger2012,
	abstract = {As a core robotic and vision problem, camera and range sensor calibration have been researched intensely over the last decades. However, robotic research efforts still often get heavily delayed by the requirement of setting up a calibrated system consisting of multiple cameras and range measurement units. With regard to removing this burden, we present a toolbox with web interface for fully automatic camera- to-camera and camera-to-range calibration. Our system is easy to setup and recovers intrinsic and extrinsic camera parameters as well as the transformation between cameras and range sensors within one minute. In contrast to existing calibration approaches, which often require user intervention, the proposed method is robust to varying imaging conditions, fully automatic, and easy to use since a single image and range scan proves sufficient for most calibration scenarios. Experimentally, we demonstrate that the proposed checkerboard corner detector significantly outperforms current state-of-the-art. Furthermore, the proposed camera-to-range registration method is able to discover multiple solutions in the case of ambiguities. Experi- ments using a variety of sensors such as grayscale and color cameras, the Kinect 3D sensor and the Velodyne HDL-64 laser scanner show the robustness of our method in different indoor and outdoor settings and under various lighting conditions. I.},
	author = {Geiger, Andreas and Moosmann, Frank and Car, {\"{O}}mer and Schuster, Bernhard},
	doi = {10.1109/ICRA.2012.6224570},
	file = {:home/nisaak/Downloads/Geiger2012ICRA.pdf:pdf},
	isbn = {9781467314039},
	issn = {10504729},
	journal = {Proceedings - IEEE International Conference on Robotics and Automation},
	mendeley-groups = {projects/proj-sp-isaak},
	pages = {3936--3943},
	title = {{Automatic camera and range sensor calibration using a single shot}},
	year = {2012}
}

@article{Geiger2012,
	abstract = {As a core robotic and vision problem, camera and range sensor calibration have been researched intensely over the last decades. However, robotic research efforts still often get heavily delayed by the requirement of setting up a calibrated system consisting of multiple cameras and range measurement units. With regard to removing this burden, we present a toolbox with web interface for fully automatic camera- to-camera and camera-to-range calibration. Our system is easy to setup and recovers intrinsic and extrinsic camera parameters as well as the transformation between cameras and range sensors within one minute. In contrast to existing calibration approaches, which often require user intervention, the proposed method is robust to varying imaging conditions, fully automatic, and easy to use since a single image and range scan proves sufficient for most calibration scenarios. Experimentally, we demonstrate that the proposed checkerboard corner detector significantly outperforms current state-of-the-art. Furthermore, the proposed camera-to-range registration method is able to discover multiple solutions in the case of ambiguities. Experi- ments using a variety of sensors such as grayscale and color cameras, the Kinect 3D sensor and the Velodyne HDL-64 laser scanner show the robustness of our method in different indoor and outdoor settings and under various lighting conditions. I.},
	author = {Geiger, Andreas and Moosmann, Frank and Car, {\"{O}}mer and Schuster, Bernhard},
	doi = {10.1109/ICRA.2012.6224570},
	file = {:home/nisaak/Downloads/Geiger2012ICRA.pdf:pdf},
	isbn = {9781467314039},
	issn = {10504729},
	journal = {Proceedings - IEEE International Conference on Robotics and Automation},
	mendeley-groups = {projects/proj-sp-isaak},
	pages = {3936--3943},
	title = {{Automatic camera and range sensor calibration using a single shot}},
	year = {2012}
}

@article{Geiger2012,
	abstract = {As a core robotic and vision problem, camera and range sensor calibration have been researched intensely over the last decades. However, robotic research efforts still often get heavily delayed by the requirement of setting up a calibrated system consisting of multiple cameras and range measurement units. With regard to removing this burden, we present a toolbox with web interface for fully automatic camera- to-camera and camera-to-range calibration. Our system is easy to setup and recovers intrinsic and extrinsic camera parameters as well as the transformation between cameras and range sensors within one minute. In contrast to existing calibration approaches, which often require user intervention, the proposed method is robust to varying imaging conditions, fully automatic, and easy to use since a single image and range scan proves sufficient for most calibration scenarios. Experimentally, we demonstrate that the proposed checkerboard corner detector significantly outperforms current state-of-the-art. Furthermore, the proposed camera-to-range registration method is able to discover multiple solutions in the case of ambiguities. Experi- ments using a variety of sensors such as grayscale and color cameras, the Kinect 3D sensor and the Velodyne HDL-64 laser scanner show the robustness of our method in different indoor and outdoor settings and under various lighting conditions. I.},
	author = {Geiger, Andreas and Moosmann, Frank and Car, {\"{O}}mer and Schuster, Bernhard},
	doi = {10.1109/ICRA.2012.6224570},
	file = {:home/nisaak/Downloads/Geiger2012ICRA.pdf:pdf},
	isbn = {9781467314039},
	issn = {10504729},
	journal = {Proceedings - IEEE International Conference on Robotics and Automation},
	mendeley-groups = {projects/proj-sp-isaak},
	pages = {3936--3943},
	title = {{Automatic camera and range sensor calibration using a single shot}},
	year = {2012}
}

@article{Geiger2012,
	abstract = {As a core robotic and vision problem, camera and range sensor calibration have been researched intensely over the last decades. However, robotic research efforts still often get heavily delayed by the requirement of setting up a calibrated system consisting of multiple cameras and range measurement units. With regard to removing this burden, we present a toolbox with web interface for fully automatic camera- to-camera and camera-to-range calibration. Our system is easy to setup and recovers intrinsic and extrinsic camera parameters as well as the transformation between cameras and range sensors within one minute. In contrast to existing calibration approaches, which often require user intervention, the proposed method is robust to varying imaging conditions, fully automatic, and easy to use since a single image and range scan proves sufficient for most calibration scenarios. Experimentally, we demonstrate that the proposed checkerboard corner detector significantly outperforms current state-of-the-art. Furthermore, the proposed camera-to-range registration method is able to discover multiple solutions in the case of ambiguities. Experi- ments using a variety of sensors such as grayscale and color cameras, the Kinect 3D sensor and the Velodyne HDL-64 laser scanner show the robustness of our method in different indoor and outdoor settings and under various lighting conditions. I.},
	author = {Geiger, Andreas and Moosmann, Frank and Car, {\"{O}}mer and Schuster, Bernhard},
	doi = {10.1109/ICRA.2012.6224570},
	file = {:home/nisaak/Downloads/Geiger2012ICRA.pdf:pdf},
	isbn = {9781467314039},
	issn = {10504729},
	journal = {Proceedings - IEEE International Conference on Robotics and Automation},
	mendeley-groups = {projects/proj-sp-isaak},
	pages = {3936--3943},
	title = {{Automatic camera and range sensor calibration using a single shot}},
	year = {2012}
}

@article{Geiger2012,
	abstract = {As a core robotic and vision problem, camera and range sensor calibration have been researched intensely over the last decades. However, robotic research efforts still often get heavily delayed by the requirement of setting up a calibrated system consisting of multiple cameras and range measurement units. With regard to removing this burden, we present a toolbox with web interface for fully automatic camera- to-camera and camera-to-range calibration. Our system is easy to setup and recovers intrinsic and extrinsic camera parameters as well as the transformation between cameras and range sensors within one minute. In contrast to existing calibration approaches, which often require user intervention, the proposed method is robust to varying imaging conditions, fully automatic, and easy to use since a single image and range scan proves sufficient for most calibration scenarios. Experimentally, we demonstrate that the proposed checkerboard corner detector significantly outperforms current state-of-the-art. Furthermore, the proposed camera-to-range registration method is able to discover multiple solutions in the case of ambiguities. Experi- ments using a variety of sensors such as grayscale and color cameras, the Kinect 3D sensor and the Velodyne HDL-64 laser scanner show the robustness of our method in different indoor and outdoor settings and under various lighting conditions. I.},
	author = {Geiger, Andreas and Moosmann, Frank and Car, {\"{O}}mer and Schuster, Bernhard},
	doi = {10.1109/ICRA.2012.6224570},
	file = {:home/nisaak/Downloads/Geiger2012ICRA.pdf:pdf},
	isbn = {9781467314039},
	issn = {10504729},
	journal = {Proceedings - IEEE International Conference on Robotics and Automation},
	mendeley-groups = {projects/proj-sp-isaak},
	pages = {3936--3943},
	title = {{Automatic camera and range sensor calibration using a single shot}},
	year = {2012}
}
@article{Geiger2012,
	abstract = {As a core robotic and vision problem, camera and range sensor calibration have been researched intensely over the last decades. However, robotic research efforts still often get heavily delayed by the requirement of setting up a calibrated system consisting of multiple cameras and range measurement units. With regard to removing this burden, we present a toolbox with web interface for fully automatic camera- to-camera and camera-to-range calibration. Our system is easy to setup and recovers intrinsic and extrinsic camera parameters as well as the transformation between cameras and range sensors within one minute. In contrast to existing calibration approaches, which often require user intervention, the proposed method is robust to varying imaging conditions, fully automatic, and easy to use since a single image and range scan proves sufficient for most calibration scenarios. Experimentally, we demonstrate that the proposed checkerboard corner detector significantly outperforms current state-of-the-art. Furthermore, the proposed camera-to-range registration method is able to discover multiple solutions in the case of ambiguities. Experi- ments using a variety of sensors such as grayscale and color cameras, the Kinect 3D sensor and the Velodyne HDL-64 laser scanner show the robustness of our method in different indoor and outdoor settings and under various lighting conditions. I.},
	author = {Geiger, Andreas and Moosmann, Frank and Car, {\"{O}}mer and Schuster, Bernhard},
	doi = {10.1109/ICRA.2012.6224570},
	file = {:home/nisaak/Downloads/Geiger2012ICRA.pdf:pdf},
	isbn = {9781467314039},
	issn = {10504729},
	journal = {Proceedings - IEEE International Conference on Robotics and Automation},
	mendeley-groups = {projects/proj-sp-isaak},
	pages = {3936--3943},
	title = {{Automatic camera and range sensor calibration using a single shot}},
	year = {2012}
}

@article{Geiger2012,
	abstract = {As a core robotic and vision problem, camera and range sensor calibration have been researched intensely over the last decades. However, robotic research efforts still often get heavily delayed by the requirement of setting up a calibrated system consisting of multiple cameras and range measurement units. With regard to removing this burden, we present a toolbox with web interface for fully automatic camera- to-camera and camera-to-range calibration. Our system is easy to setup and recovers intrinsic and extrinsic camera parameters as well as the transformation between cameras and range sensors within one minute. In contrast to existing calibration approaches, which often require user intervention, the proposed method is robust to varying imaging conditions, fully automatic, and easy to use since a single image and range scan proves sufficient for most calibration scenarios. Experimentally, we demonstrate that the proposed checkerboard corner detector significantly outperforms current state-of-the-art. Furthermore, the proposed camera-to-range registration method is able to discover multiple solutions in the case of ambiguities. Experi- ments using a variety of sensors such as grayscale and color cameras, the Kinect 3D sensor and the Velodyne HDL-64 laser scanner show the robustness of our method in different indoor and outdoor settings and under various lighting conditions. I.},
	author = {Geiger, Andreas and Moosmann, Frank and Car, {\"{O}}mer and Schuster, Bernhard},
	doi = {10.1109/ICRA.2012.6224570},
	file = {:home/nisaak/Downloads/Geiger2012ICRA.pdf:pdf},
	isbn = {9781467314039},
	issn = {10504729},
	journal = {Proceedings - IEEE International Conference on Robotics and Automation},
	mendeley-groups = {projects/proj-sp-isaak},
	pages = {3936--3943},
	title = {{Automatic camera and range sensor calibration using a single shot}},
	year = {2012}
}

@article{DBLP:journals/corr/DhallCRK17,
	author    = {Ankit Dhall and
	Kunal Chelani and
	Vishnu Radhakrishnan and
	K. M. Krishna},
	title     = {LiDAR-Camera Calibration using 3D-3D Point correspondences},
	journal   = {CoRR},
	volume    = {abs/1705.09785},
	year      = {2017},
	url       = {http://arxiv.org/abs/1705.09785},
	archivePrefix = {arXiv},
	eprint    = {1705.09785},
	timestamp = {Mon, 13 Aug 2018 16:46:43 +0200},
	biburl    = {https://dblp.org/rec/bib/journals/corr/DhallCRK17},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Ortiz2018,
	abstract = {The ZED camera is binocular vision system that can be used to provide a 3D perception of the world. It can be applied in autonomous robot navigation, virtual reality, tracking, motion analysis and so on. This paper proposes a mathematical error model for depth data estimated by the ZED camera with its several resolutions of operation. For doing that, the ZED is attached to a Nvidia Jetson TK1 board providing an embedded system that is used for processing raw data acquired by ZED from a 3D checkerboard. Corners are extracted from the checkerboard using RGB data, and a 3D reconstruction is done for these points using disparity data calculated from the ZED camera, coming up with a partially ordered, and regularly distributed (in 3D space) point cloud of corners with given coordinates (x e , y e , z e), which are computed by the device software. These corners also have their ideal world (3D) positions (x i , y i , z i) known with respect to the coordinate frame origin that is empirically set in the pattern. Both given (computed) coordinates from the camera's data and known (ideal) coordinates of a corner can, thus, be compared for estimating the error between the given and ideal point locations of the detected corner cloud. Subsequently, using a curve fitting technique, we obtain the equations that model the RMS (Root Mean Square) error. This procedure is repeated for several resolutions of the ZED sensor, and at several distances. Results showed its best effectiveness with a maximum distance of approximately sixteen meters, in real time, which allows its use in robotic or other online applications.},
	author = {Ortiz, Luis Enrique and Cabrera, Viviana Elizabeth and Goncalves, Luiz M G},
	doi = {10.5565/rev/elcvia.1084},
	file = {:home/nisaak/Downloads/elcvia{\_}a2018v17n1p1.pdf:pdf},
	journal = {ELCVIA Electronic Letters on Computer Vision and Image Analysis},
	keywords = {depth data,rms error,stereo vision,stereolabs zed},
	mendeley-groups = {projects/proj-sp-isaak},
	number = {1},
	pages = {1},
	title = {{Depth Data Error Modeling of the ZED 3D Vision Sensor from Stereolabs}},
	volume = {17},
	year = {2018}
}



@article{DBLP:journals/corr/abs-1803-08669,
	author    = {Jia{-}Ren Chang and
	Yong{-}Sheng Chen},
	title     = {Pyramid Stereo Matching Network},
	journal   = {CoRR},
	volume    = {abs/1803.08669},
	year      = {2018},
	url       = {http://arxiv.org/abs/1803.08669},
	archivePrefix = {arXiv},
	eprint    = {1803.08669},
	timestamp = {Mon, 13 Aug 2018 16:48:22 +0200},
	biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1803-08669},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Gao2011,
	abstract = {This paper presents an obstacle detection system which is robust to non-flat road surface and interference of illumination. A 3D camera is used to generate depth information without the need of camera calibration. The depth map is then transformed into U-V-disparity domain, where obstacles and ground surface are projected as lines. Hough Transform is employed to extract line features; it has been modified to fit the characteristic of the U-V-disparity in order to boost the speed and accuracy. In addition, steerable filters are applied to the u-v histogram before Hough Transform for noise reduction. By categorising extracted lines according to their position and posture, road surface and on-road obstacles can be detected. Finally, results obtained using both U and V disparity maps are combined to eliminate road side surface and post processing. Experiments show that the proposed system is Able to detect obstacles accurately under various environments.},
	author = {Gao, Y. and Ai, X. and Wang, Y. and Rarity, J. and Dahnoun, N.},
	doi = {10.1109/IVS.2011.5940425},
	file = {:home/nisaak/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gao et al. - 2011 - U-V-disparity based obstacle detection with 3D camera and steerable filter.pdf:pdf},
	isbn = {9781457708909},
	journal = {IEEE Intelligent Vehicles Symposium, Proceedings},
	mendeley-groups = {projects/proj-sp-isaak},
	number = {Iv},
	pages = {957--962},
	title = {{U-V-disparity based obstacle detection with 3D camera and steerable filter}},
	year = {2011}
}
