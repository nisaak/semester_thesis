% !TeX spellcheck = en_US
% !TeX encoding = UTF-8
% !TeX root = ../report.tex

\chapter{Method}
\label{chp:Method}

\section{V-Disparity}

This chapter explains the theoretical basis of the V-disparity method. To begin with, the basis of the V-Disparity method, the disparity map, is outlined.

\subsection{Disparity Map} \label{sbs:method_dispmap}
It is assumed, that the reader has a general understanding of stereovision systems. Given two calibrated stereo cameras, the respective rectified images can be used to calibrate a disparity for each pixel. These is achieved with different block-matching algorithms, such as the StereoSGBM algorithm implemented in the popular OpenCV library. Given a disparity map, the baseline and focal length, a depth map can be calculated and used for further processing. The V-Disparity method however only makes use of the disparity map.


%Elaborate on calculation and formulas of disparity

\subsection{V-Disparity}

Once a disparity map $\Delta$(u,v) was computed, a V-Disparity histogram can be constructed. For each row u, a histogram of the occuring disparities in this row is computed. The histogram values represent the occurence of the respective disparity in the row, where each bin is represented by a pixel. Given a plane in a scene, the projection of the plane onto the V-Disparity image has a useful property. 
A plane will be projected as a linear curve in the V-Disparity image. This simplifies the extraction of the respective plane in the V-Disparity image, as a e.g. Hough Line Transform can be applied to detect the lines. Consequently, the detection of straight lines in the V-Disparity corresponds to detection of planes in the scene.
A scene is therefore made up of planes, where vertical planes can be understood as obstacles, horizontal planes as the road when flat, or as a set of oblique planes when the road is non-flat. Hu et al. \cite{Hu2005} offer an in-depth analysis of the projection of the three above types of planes.

\newline

Because the most prominent plane in usually represented by the ground, it will be detected as the line with the most votes in the V-Disparity image. Horizontal or vertical lines can be dismissed, as the disparity gradient of the road leads to a skewed line.
Horizontal lines can be associated with obstacles and used for obstacle detection.
Even though Non-flat road geometry will not be considered in this project, it should be noted that the road in that case can be approximated by a series of planes, which will then be projected as a piecewise linear curve in the V-Disparity image.

Once the line has been fitted in the V-Disparity image, the disparity values for the road surface are known. Extracting the road in the image domain is straightforward. For each row, the values which lie within a threshold of the value of the extracted line are part of the road, all other pixels are masked as non-road.

\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{Figures/vdisp}
	\caption[Overview of V-Disparity method]{}
	\label{fig:vdisp}
\end{figure}

Oniga et al. \cite{Oniga2015} show a camera image, the corresponding dispartiy map and the V- and U-Disparity respectively.



\section{Lidar Camera Projection}

%Write about how projection is done, how matrices are found and structure.
The lidar scans its surrounding and generates a point cloud of the structures and objects around it. To begin with, a simple bidary classification was put in place. The classifier distinguishes obstacles and ground, via a simple height threshold. Therefore, all points above a certain height threshold are classified as obstacles. This is a rough approximation and by no means sufficient for higher level classification and detection task, but for a first implementation of the proposed method it adequate.
If one only wants to project the points belonging to the ground onto the camera frame, the point cloud can be compressed to a 2D occupancy map, which will be used later. The full point cloud can be used as well, if needed. At first, the point cloud and occupancy map will be sparse, after a few laps however, it should be dense and contain all relevant points and it doesn't need to be updated anymore.
After the point cloud/occupancy map acquisition, the points can be projected onto the camera frame.

Before a point in camera coordinates can be projected onto the camera frame, the respective point in world coordinates needs to be converted to camera coordinates. \newline
Let $X_{w}$  be a point in world coordinate frame. \newline
Let $X_{c}$ be a a point in camera coordinate frame. \newline
The two points relate to each other by a translation and rotation. To convert $X_{w}$ to $X_{c}$, a translation and rotation, in that order, need to be applied. 

\begin{equation} \label{eq:1}
X_{c} = R(X_{w} - C)
\end{equation}


Where all points are in non-homogeneous coordinates, R and C with respect to the world coordinate system.

In homogenous coordinates, equation \ref{eq:1} can be written as follows.

\begin{equation} \label{eq:2}
	\widetilde{X}_{c} = \begin{bsmallmatrix} R & -RC \\ 0 & 1  \end{bsmallmatrix} \widetilde{X}_{w}
\end{equation}

To project lidar scanning points onto a camera image, the camera's intrinsic parameters should be known, namely the focal length $f_{x}$ and $f_{y}$ in x and y direction respectively and the camera's optical center $c_{x}$ and $c_{y}$. All can be summarised in, what is called, the intrinsic matrix K of the camera.

\begin{equation} \label{mat:1}
K = \begin{bsmallmatrix} f_{x} & 0 & c_{x} \\ 0 & f_{y} & c_{y} \\ 0 & 0 & 1  \end{bsmallmatrix}
\end{equation}

For most cameras, the distortion parameters need to be considered, to correct the pixel projection.
OpenCV considers radial and tangential factors. More on that can be found on the OpenCV documentation site.

The final projection of the point in camera coordinates onto the camera frame is done via simple matrix multiplication.

\begin{equation} \label{eq:3}
x = K [I | 0] \widetilde{X}_{c} = K R[I | -C] \widetilde{X}_{w}
\end{equation}

Where K denotes the intrinsic matrix from \ref{mat:1} and I the identity matrix.

%write about projection matrix

%Additionally, the projection matrix from lidar to camera needs to be determined. It can be measured by hand and computed, or determined by externally calibrating the lidar and camera with an automated pipeline.

%While driving on our indoor Gokart track, the lidar generates an occupancy map. This occupancy map is then projected onto the camera frame. By determining the relative pose and orientation of the camera with respect to the world frame, one can then project all points corresponding to the road onto the camera frame.

To determine the rotation and translation matrix, the kart's position and orientation with respect to the world coordinate frame needs to be known. Also, the relativ position and orientation of the camera to the kart's center needs to be measured or determined by a different method. Measuring can lead to large projection errors, as the method is inherently sensitive to even small errors. A different method to find the respective rotation and translation matrix is proposed by Dhall et al. \cite{DBLP:journals/corr/DhallCRK17}.

All points which belong to the ground and are projected onto the camera image can be interpreted as a mask, covering all points in the camera image which belong to the ground. 