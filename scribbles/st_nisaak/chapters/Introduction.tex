% !TeX spellcheck = en_US
% !TeX encoding = UTF-8
% !TeX root = ../report.tex

\chapter{Introduction}
\label{chp:Introduction}

%Talking points
%road segmentation, obstacle detection, autonomous driving	

The task of segmenting a scene into ground, obstacles and other labels is a well-researched topic. It is a fundamental part of any autonomous ground vehicle, and lays the basis for many different tasks, such as planning, safety features or scene understanding. Over the last few years, solutions to this task, which are based on machine learning methods, have deemed themselves to be robust and accurate. Their ability to generalise and be applied to a variety of scenes make them a reliable choice. There however, exist many approaches to the problem, which are based on classic computer vision, with implementations dating many years back. \newline
The goal of this semester project is the implementation of a robust and accurate free space detection pipeline based on classic computer vision. The results of this pipeline can be used to e.g. label data for machine learning. If the pipeline does not perform as expected, a second pipeline is introduced, which in a broader sense is a external calibration of the lidar and the onboard camera. The need for a lidar sensor highlights the necessity of multiple sensors to create a robust pipeline.

This semester project is part of a more extensive research into autonomous driving, based a self-driving Gokart. The Gokart is basis for a variety of research topics, such as planning, control systems and sensor fusion. The testing environment is a modular indoor Gokart track. The ground is flat, with no uphill or downhill sections. The track is not affected by weather conditions. 
\newline


\section{Related Work}

\subsection{V-Disparity}
Related work on this topic is extensive. After limiting my research to the method of V-Disparity, early implementations can be dated back to 2002, with Labayrade et al. \cite{V-disparity_nonflat} providing a robust and accurate method for road detection, even for non-flat geometries. A study on the U-V Disparity method can be found in \cite{Hu2005}, providing a real-time implementation of the stereovision based scene analysis. The method was improved and adapted since it's introduction, becoming more robust and computationally efficient, as shown in \cite{Kakegawa2018}. Aside from free space detection, U-V-Disparity can be used solely for obstacle and pedestrian detection, with U-V-Disparity acting as the underlying basis for a SVM Classifier, where the extraced ROIs are used for training. Iloie et al. implemented this framework in \cite{Iloie2014}.

\subsection{Lidar to camera projection}
Different approaches exist to calibrate a range sensor, in our case a Velodyne Vld-16 Lidar, with a camera. Geiger et al. \cite{Geiger2012} propose an automatic camera and range sensor calibration, using checkerboard patterns using 3D correspondence matching. A refined method, proposed by Dhall et al. \cite{DBLP:journals/corr/DhallCRK17}, uses Aruco markers and 3D-3D point correspondences, to increase the accuracy and robustness of the method. The code implementation of this method is available as a ROS package on https://github.com/ankitdhall/lidar_camera_calibration.

\section{Structure of this report}
This report is structured as follows. In \ref{chp:Method} both, the V-Disparity method and Lidar to Camera projection are explained in theory. Section \ref{chp:Implementation} discusses the implementation of the above methods, relevant parts of the code and additionally the mounted hardware.
Results are presented in section \ref{chp:Results}, followed by a conclusion and proposed future work in \ref{chp:Conclusion}.
